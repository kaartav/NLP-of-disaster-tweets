{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81299d54-acc8-43f5-acb3-5dd5ae12088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d02a9d1-b671-4b28-9b6e-efac9e233fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train.csv\")\n",
    "test=pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec5c46a-c3b1-4488-94a3-1828ba18c076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_namesssss = df.columns\n",
    "\n",
    "col_namesssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87479b24-dc49-41a3-9660-59eb00b586c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "None\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n",
      "None\n",
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.head())\n",
    "print(test.info())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec943eb1-e069-4347-90df-befbf37cf5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = df[\"target\"]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0f5342-03a3-4231-8b96-78381f2452dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].isnull().sum()\n",
    "test[\"text\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b56dd64-cbfc-46de-a28a-3e674394a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(\"none\", inplace=True)\n",
    "test.fillna(\"none\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5843fdba-9d86-4f61-b2c2-e09f3facb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace URLs with 'webaddress'\n",
    "df['text'] = df['text'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddress')\n",
    "test['text'] = test['text'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddress')                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4c7855-fb14-4118-9015-a21d874a11aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.005  # You can adjust this threshold\n",
    "\n",
    "# # Calculate the frequency of each value\n",
    "# value_counts = df['location'].value_counts()\n",
    "\n",
    "# # Identify values to group into \"Other\"\n",
    "# values_to_group = value_counts[value_counts / len(df) < threshold].index\n",
    "\n",
    "# # Replace values in the DataFrame with \"Other\"\n",
    "# df['location'] = df['location'].apply(lambda x: x if x not in values_to_group else 'Other')\n",
    "\n",
    "# # Perform one-hot encoding\n",
    "# one_hot = pd.get_dummies(df['location'], prefix='location')\n",
    "\n",
    "# # Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "# df = pd.concat([df, one_hot], axis=1)\n",
    "\n",
    "# # Drop the original 'countries' column\n",
    "df.drop(['location','keyword'], axis=1, inplace=True)\n",
    "test.drop(['location','keyword'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfcf41c-b54d-45ab-b74d-10c7eaff9fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          id                                               text  target\n",
       "0         1  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1         4             Forest fire near La Ronge Sask. Canada       1\n",
       "2         5  All residents asked to 'shelter in place' are ...       1\n",
       "3         6  13,000 people receive #wildfires evacuation or...       1\n",
       "4         7  Just got sent this photo from Ruby #Alaska as ...       1\n",
       "...     ...                                                ...     ...\n",
       "7608  10869  Two giant cranes holding a bridge collapse int...       1\n",
       "7609  10870  @aria_ahrary @TheTawniest The out of control w...       1\n",
       "7610  10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "7611  10872  Police investigating after an e-bike collided ...       1\n",
       "7612  10873  The Latest: More Homes Razed by Northern Calif...       1\n",
       "\n",
       "[7613 rows x 3 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "725f530e-74bb-4935-8dad-240ba364f582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          id                                               text\n",
       "0         0                 Just happened a terrible car crash\n",
       "1         2  Heard about #earthquake is different cities, s...\n",
       "2         3  there is a forest fire at spot pond, geese are...\n",
       "3         9           Apocalypse lighting. #Spokane #wildfires\n",
       "4        11      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "...     ...                                                ...\n",
       "3258  10861  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
       "3259  10865  Storm in RI worse than last hurricane. My city...\n",
       "3260  10868  Green Line derailment in Chicago http://t.co/U...\n",
       "3261  10874  MEG issues Hazardous Weather Outlook (HWO) htt...\n",
       "3262  10875  #CityofCalgary has activated its Municipal Eme...\n",
       "\n",
       "[3263 rows x 2 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8926887c-3d14-471b-ba0d-cc2e18b6cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform one-hot encoding\n",
    "# one_hot = pd.get_dummies(df['keyword'], prefix='keyword')\n",
    "\n",
    "# # Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "# df = pd.concat([df, one_hot], axis=1)\n",
    "\n",
    "# # Drop the original 'countries' column\n",
    "# df.drop(['keyword'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f818de2b-c93f-4d22-82c9-39bd4ff30512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1   4             Forest fire near La Ronge Sask. Canada       1\n",
       "2   5  All residents asked to 'shelter in place' are ...       1\n",
       "3   6  13,000 people receive #wildfires evacuation or...       1\n",
       "4   7  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d097c2c-ccd3-443b-8cbd-79eebe829b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f96725-344e-4ef5-b683-3b259ec7c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modification_tweet(tweet):\n",
    "    s=re.sub('[^a-zA-Z]',' ',tweet)\n",
    "    s=s.lower()\n",
    "    s=s.split()\n",
    "    s=[ps.stem(word) for word in s if not word in set(stopwords.words('english'))]\n",
    "    s=' '.join(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c0762c5-968f-406c-904f-173edba459ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', \"couldn't\", 'it', 'we', 've', 'further', 'because', 'yourselves', 'between', 'up', 'this', 'do', 'just', 'when', \"you'll\", 'does', 'me', 'were', 'down', 'he', 'hasn', 'has', 'doing', \"don't\", 'or', 'shouldn', 'itself', 'so', 'more', \"should've\", 'once', 'there', 'herself', 'about', 'ours', 'didn', \"didn't\", 'isn', 'm', \"it's\", 'hers', 'by', 'are', 'that', \"needn't\", 'wasn', 'if', \"isn't\", 'your', 'out', 'their', 'here', 'ain', 'been', 'did', 'both', 'mustn', 'very', 're', 'was', 'i', 'its', 'couldn', 'but', 'hadn', 'ma', 'don', 'some', 'such', 'my', 'from', 'd', 'with', 'as', \"you've\", 'any', 'her', 'while', 'than', 'them', 'who', 'what', 'nor', 'an', \"haven't\", 'then', 'each', \"hasn't\", 'have', 'of', 'all', 'having', 'weren', 'myself', 'she', \"wouldn't\", 'no', 'during', 's', 'be', \"mustn't\", 'you', 'own', 'himself', 'shan', 'these', 'for', 'which', 'can', 'll', 'the', 'yourself', 'whom', 'why', 'theirs', 'our', 'mightn', 'had', 'ourselves', 'is', 'off', 'doesn', 'under', \"you're\", 'to', 't', 'below', 'should', 'him', 'until', 'they', 'those', 'at', 'being', 'against', 'wouldn', 'where', 'and', 'again', 'needn', \"weren't\", 'few', \"mightn't\", 'most', 'am', 'in', 'too', 'haven', 'aren', 'themselves', \"won't\", 'will', 'on', 'how', 'only', 'same', \"shouldn't\", \"hadn't\", \"shan't\", 'his', 'through', 'not', \"she's\", \"aren't\", \"you'd\", \"wasn't\", \"that'll\", 'after', 'other', 'before', 'yours', 'into', 'y', 'above', 'over', 'o', \"doesn't\", 'won', 'now'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1c6774a-1a0f-43de-9838-beb60064665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"]=df['text'].apply(modification_tweet)\n",
    "test[\"text\"]=test['text'].apply(modification_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd65859-86f5-479c-8331-000a6928a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06c1ce74-395a-4a26-95d0-5af33c462cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=[]\n",
    "# for string in df[\"text\"]:\n",
    "#     s=string.split()\n",
    "#     corpus.append(s)\n",
    "\n",
    "# model=Word2Vec(corpus,min_count=1)\n",
    "# # Access the vocabulary using index_to_key and key_to_index\n",
    "# index_to_key = model.wv.index_to_key  # List of words\n",
    "# key_to_index = model.wv.key_to_index  # Dictionary mapping words to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db9ab66c-9585-4703-83a8-723b93c9f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Word2Vector(modified_tweet):\n",
    "#     s=modified_tweet.split()\n",
    "#     # vec_size=wv.vector_size\n",
    "#     res=np.zeros(100)\n",
    "#     n=0\n",
    "#     for w in s:\n",
    "#         if w in index_to_key:\n",
    "#             n+=1\n",
    "#             word_index = key_to_index[w]\n",
    "#             res=res+model.wv.vectors[word_index]\n",
    "#     res=res/n\n",
    "#     return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6249bb98-ca71-42e2-8c8e-085ca8cf6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['vector']=df['text'].apply(Word2Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b234b25-a5c0-4627-b1d0-1f21ba33c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=df['target']\n",
    "# df.drop(['target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d23610-bd04-4591-ad17-0d100cbfe844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df['vector'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38f8896f-2a8e-4798-a207-8dce1fbc0805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1          deed reason earthquak may allah forgiv us       1\n",
       "1   4               forest fire near la rong sask canada       1\n",
       "2   5  resid ask shelter place notifi offic evacu she...       1\n",
       "3   6        peopl receiv wildfir evacu order california       1\n",
       "4   7  got sent photo rubi alaska smoke wildfir pour ...       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44456339-d070-4871-b504-172ed2df0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['text'], axis=1, inplace=True)\n",
    "# df.drop(['vector'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afaf7044-ed11-4468-961a-e056376c991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x = pd.DataFrame(x)\n",
    "\n",
    "# # Optionally, you can specify column names for the DataFrame\n",
    "# # If each vector has the same number of dimensions, you can name the columns like 'dim_0', 'dim_1', ...\n",
    "# # For example, if you have 100-dimensional vectors:\n",
    "# column_names = [f'dim_{i}' for i in range(100)]\n",
    "# df_x.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8324ff2-afe5-4642-8eb2-67d23208cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.join(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "999b83e8-e034-4887-9b82-6ca5b570cfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1          deed reason earthquak may allah forgiv us       1\n",
       "1   4               forest fire near la rong sask canada       1\n",
       "2   5  resid ask shelter place notifi offic evacu she...       1\n",
       "3   6        peopl receiv wildfir evacu order california       1\n",
       "4   7  got sent photo rubi alaska smoke wildfir pour ...       1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17e9c625-0ac2-4161-b70d-b3e8434708fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['dim_90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2ef29bf-f68a-43d4-9185-e43aad04f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df*1\n",
    "# df.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04a59d6c-648f-47ae-ab71-b74e95314cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the training data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    df['text'],  # Input features (preprocessed text)\n",
    "    df['target'],  # Target variable\n",
    "    test_size=0.2,  # Validation set size\n",
    "    random_state=42  # Seed for reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "039256b2-db1c-4899-aabd-c2b4ed621e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(1, 2))  # Maximum number of features,  \n",
    "                                   # Include unigrams and bigrams\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Transform the test data (using the same vectorizer)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1433fd60-7695-4170-a4f6-199d9ec71398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 5000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c701bdbf-cb19-42f5-b2d3-dd556dc95028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "969816d9-ca78-49c3-a618-be4be0f4e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Initialize the Logistic Regression model\n",
    "# model = LogisticRegression()\n",
    "\n",
    "# # Train the model on the TF-IDF features and target variable\n",
    "# model.fit(X_train_tfidf,y_train)\n",
    "\n",
    "# # Predict on the validation set\n",
    "# y_pred = model.predict(X_valid_tfidf)\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# test_predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_valid, y_pred)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3f17b71-61cf-47f4-88a7-78b79d226e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = SVC()\n",
    "# logistic_regression_classifier = LogisticRegression()\n",
    "# # multinomial_nb_classifier = MultinomialNB()\n",
    "\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "# logistic_regression_classifier.fit(X_train, y_train)\n",
    "# # multinomial_nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22e4473e-b38b-4acd-85cb-9a08f15cb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "# # sb.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
    "# axes[0, 1].set_title('Confusion Matrix')\n",
    "# axes[0, 1].set_xlabel('Predicted Labels')\n",
    "# axes[0, 1].set_ylabel('True Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6959a1ae-ab2f-4e89-af7b-3e5a95e7cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_classifier.predict(X_valid_tfidf)\n",
    "\n",
    "# logistic_regression_predictions = logistic_regression_classifier.predict(X_valid_tfidf)\n",
    "# multinomial_nb_predictions = multinomial_nb_classifier.predict(X_test)\n",
    "# Make predictions on the test data\n",
    "test_predictions_svm = svm_classifier.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e84198e6-83d3-4c7b-b3f8-05245b8d51f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SVM:\n",
      "[[772 102]\n",
      " [204 445]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate confusion matrices\n",
    "svm_cm = confusion_matrix(y_valid, svm_predictions)\n",
    "# lr_cm = confusion_matrix(y_valid, y_pred)\n",
    "# multinomial_nb_cm = confusion_matrix(y_test, multinomial_nb_predictions)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix for SVM:\")\n",
    "print(svm_cm)\n",
    "# print(\"nConfusion Matrix for Logistic Regression:\")\n",
    "# print(lr_cm)\n",
    "# # print(\"\\nConfusion Matrix for Multinomial Naive Bayes:\")\n",
    "# print(multinomial_nb_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d792c1ac-6437-41ca-9321-83ff0a14bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy = accuracy_score(y_test,logistic_regression_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a81e02dc-3d95-4ce2-810a-48d6a266f8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.799080761654629"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_valid,svm_predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da52de54-d62a-4b78-8567-fdb800c522ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame for the submission\n",
    "# submission_df = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n",
    "# submission_df.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34aca7b2-f343-4caf-b83a-ce91a6bdab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame for the submission\n",
    "submission_df_csv = pd.DataFrame({'id': test['id'], 'target': test_predictions_svm})\n",
    "submission_df_csv.to_csv('sample_submissionsvm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0081ed-2c68-4384-be7d-777af7adbd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
